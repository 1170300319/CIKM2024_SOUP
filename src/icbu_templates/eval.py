# =====================================================
#  Evaluate LLM prompts
# =====================================================

def getEvalHead():
    task_subgroup = {}

    # eval relevant and quality
    task_subgroup['1-1'] = "You will be given the interaction history and recommendation texts generated by the model for {} users. \n" \
                    "Evaluate the quality and relevance of 10 recommendation texts generated by a recommendation model to users' interaction history. \n" \
                    "Input: User's interaction history followed by recommendation texts for each user. \n" \
                    "Output: An average relevance score and an average quality score for 10 recommendation texts for each user, use 'Overall Average Relevance Score:' to output the scores. " \
                    "Then calculate the average relevance score and average quality score for all users, use 'Overall Average Quality Score:' to output the scores\n"

    # eval relevant, quality and matching
    task_subgroup['1-2'] = "You will be given the interaction history and recommendation texts generated by the model for {} users. \n" \
                    "Evaluate the quality and relevance of 10 recommendation texts generated by a recommendation model to users' interaction history. \n" \
                    "And evaluate the matching scores of 10 recommendation texts to the target query. \n" \
                    "Input: User's interaction history followed by recommendation texts for each user and corresponding target. \n" \
                    "Output: An average relevance score, an average quality score and an average matching score for 10 recommendation texts for each user. " \
                    "Use 'Overall Average Relevance Score: (). Overall Average Quality Score: (). Overall Average Matching Score: ()' to output the scores\n"

    # eval quality and matching
    task_subgroup['1-3'] = "You will be given the query ID, query title, keywords and recommendation texts generated by the model for {} users. \n" \
                    "Evaluate the quality of 10 recommendation texts generated by the recommendation model. \n" \
                    "And evaluate the matching scores of 10 recommendation texts to the target query. \n" \
                    "Input: query ID, title and keywords followed by recommendation texts for each user and corresponding target. \n" \
                    "Output: An average quality score and an average matching score for 10 recommendation texts for each user. " \
                    "Use 'Overall Average Quality Score: (). Overall Average Matching Score: ()' to output the scores\n"

    task_subgroup['1-4'] = "You will be given a item title, a item category and category prediction from a language model. \n" \
                            "Evaluate if the category prediction is a suitable category for the item in samantics. \n" \
                            "A item may related to multiple categories, so the prediction don't have to match the given tiem category perfectly. \n" \
                            "Input: item title, item category and category prediction. \n" \
                            "Output: yes if the category prediction is a suitable category for the item else no. \n"

    task_subgroup['1-5'] = "In a recommendation system, 'query' typically refers to the user's search query or input information." \
                            "Assuming you are a data processing worker, you will be given a query and a query category, " \
                            "predict whether the given query is a suitable data for machine learning. " \
                            "You should choose the English sentence with specific meanings " \
                            "If your answer is 'Yes', then given a relevant scores {0 or 1} for the category to the query. " \
                            "Note that 0 stands for irrelevant, 1 stands for relevant. "

    return task_subgroup


def getEvalCriteria():
    task_subgroup = {}

    # eval relevant and quality
    task_subgroup['2-1'] = "We will evaluate the relevance of the recommendation texts with respect to the user's interaction history based on the following criteria: 1) lexical similarity, 2) topic matching, and 3) historical behavior matching. " \
                    "We will evaluate the quality of the recommendation texts by focusing on potential issues related to 1)multilingualism, 2)semantic errors, 3)spelling errors, 4）fluency and similar issues.\n"

    # eval relevant, quality and matching
    task_subgroup['2-2'] = "We will evaluate the relevance of the recommendation texts with respect to the user's interaction history based on the following criteria: 1) lexical similarity, 2) topic matching, and 3) historical behavior matching. " \
                    "We will evaluate the quality of the recommendation texts by focusing on potential issues related to 1)multilingualism, 2)semantic errors, 3)spelling errors, 4）fluency and similar issues.\n" \
                    "We will evaluate the matching of the recommendation texts by considering the following aspects 1)functionality, 2)quality, 3)attributes.\n"

    # eval quality and matching
    task_subgroup['2-3'] = "We will evaluate the quality of the recommendation texts by focusing on potential issues related to 1)multilingualism, 2)semantic errors, 3)spelling errors, 4）fluency and similar issues.\n" \
                    "We will evaluate the matching of the recommendation texts by considering the following aspects 1)functionality, 2)quality, 3)attributes.\n"
    return task_subgroup


def getEvalMethod():
    task_subgroup = {}

    # 5 ranks for relevant and quality
    task_subgroup['3-1'] = "The scores will range from 0 to 10 and each score can be interpreted as follows:\n" \
                    "For the relevance score:\n" \
                    "0: No relevance; the recommended text has nothing in common with the user historical sequence.\n"\
                    "1-3: Weak relevance; there are some weak connections between the recommended text and the historical sequence, such as some similar vocabulary or themes.\n"\
                    "4-6: Moderate relevance; there are some clear connections between the recommended text and the historical sequence, such as common themes and vocabulary, but there are still some mismatches.\n"\
                    "7-9: High relevance; there is a strong correlation between the recommended text and the historical sequence, such as similar themes and vocabulary, and there is already a high level of feedback on the historical sequence.\n"\
                    "10: Perfect relevance; the recommended text matches the historical sequence perfectly, with a high degree of similarity and satisfaction on the historical sequence.\n"\
                    "For the quality score:\n"\
                    "0: Very poor quality; includes serious language errors, extremely poor language expression, and incomprehensible content.\n"\
                    "1-3: Some issues; there are some language errors, vague language expression, and imperfect language fluency.\n"\
                    "4-6: Room for improvement; there are still some areas that need improvement, such as some language errors, but overall language expression is still understandable and fluent.\n"\
                    "7-9: Very high quality; only minor issues, such as some small errors or imperfect language expression.\n"\
                    "10: Perfect quality; no issues, with fluent language expression, minimal language errors, and no problems with language fluency.\n"

    # 5 ranks for relevant and 3 ranks for quality
    task_subgroup['3-2'] = "The scores will be interpreted as follows:\n" \
                    "For the relevance score:\n" \
                    "1: No relevance; the recommended text has nothing in common with the user historical sequence.\n"\
                    "2: Weak relevance; there are some weak connections between the recommended text and the historical sequence, such as some similar vocabulary or themes.\n"\
                    "3: Moderate relevance; there are some clear connections between the recommended text and the historical sequence, such as common themes and vocabulary, but there are still some mismatches.\n"\
                    "4: High relevance; there is a strong correlation between the recommended text and the historical sequence, such as similar themes and vocabulary, and there is already a high level of feedback on the historical sequence.\n"\
                    "5: Perfect relevance; the recommended text matches the historical sequence perfectly, with a high degree of similarity and satisfaction on the historical sequence.\n"\
                    "For the quality score:\n"\
                    "1: Very poor quality; includes serious language errors, extremely poor language expression, and incomprehensible content.\n"\
                    "2: Room for improvement; there are still some areas that need improvement, such as some language errors, but overall language expression is still understandable and fluent.\n"\
                    "3: Perfect quality; no issues, with fluent language expression, minimal language errors, and no problems with language fluency.\n"

    # 5 for relevant and match, 3 for quality
    task_subgroup['3-3'] = "The scores will be interpreted as follows:\n" \
                    "For the relevance score:\n" \
                    "1: No relevance; the recommended text has nothing in common with the user historical sequence.\n"\
                    "2: Weak relevance; there are some weak connections between the recommended text and the historical sequence, such as some similar vocabulary or themes.\n"\
                    "3: Moderate relevance; there are some clear connections between the recommended text and the historical sequence, such as common themes and vocabulary, but there are still some mismatches.\n"\
                    "4: High relevance; there is a strong correlation between the recommended text and the historical sequence, such as similar themes and vocabulary, and there is already a high level of feedback on the historical sequence.\n"\
                    "5: Perfect relevance; the recommended text matches the historical sequence perfectly, with a high degree of similarity and satisfaction on the historical sequence.\n"\
                    "For the quality score:\n"\
                    "1: Very poor quality; includes serious language errors, extremely poor language expression, and incomprehensible content.\n"\
                    "2: Room for improvement; there are still some areas that need improvement, such as some language errors, but overall language expression is still understandable and fluent.\n"\
                    "3: Perfect quality; no issues, with fluent language expression, minimal language errors, and no problems with language fluency.\n"\
                    "For the matching score:\n" \
                    "1: No Match; The generated result has substantial differences from the target\n" \
                    "2: Weak match; The generated result has significant differences from the target. It differs from the target in many aspects such as functionality, quality, attributes, etc. \n" \
                    "3: Moderate match; The generated result has some differences compared to the target. The similarity is moderate. The user finds it acceptable..\n" \
                    "4: High Match; The generated result is similar to the target but with slight differences. It is similar to the target in terms of functionality, quality, attributes, etc\n" \
                    "5: Perfect Match; There is almost no difference between the generated result and the target. The generated result perfectly matches the user's requirements and preferences.\n"

    # 5 for match and 3 for quality
    task_subgroup['3-4'] = "The scores will be interpreted as follows:\n" \
                    "For the quality score:\n"\
                    "1: Very poor quality; includes serious language errors, extremely poor language expression, and incomprehensible content.\n"\
                    "2: Room for improvement; there are still some areas that need improvement, such as some language errors, but overall language expression is still understandable and fluent.\n"\
                    "3: Perfect quality; no issues, with fluent language expression, minimal language errors, and no problems with language fluency.\n"\
                    "For the matching score:\n" \
                    "1: No Match; The generated result has substantial differences from the target\n" \
                    "2: Weak match; The generated result has significant differences from the target. It differs from the target in many aspects such as functionality, quality, attributes, etc. \n" \
                    "3: Moderate match; The generated result has some differences compared to the target. The similarity is moderate. The user finds it acceptable..\n" \
                    "4: High Match; The generated result is similar to the target but with slight differences. It is similar to the target in terms of functionality, quality, attributes, etc\n" \
                    "5: Perfect Match; There is almost no difference between the generated result and the target. The generated result perfectly matches the user's requirements and preferences.\n"
    return task_subgroup


def getEvalNote():
    task_subgroup = {}

    # eval relevant and quality
    task_subgroup['4-1'] = "Please note that your evaluation should be based on the above criteria and should be conducted as accurately as possible. Please carefully evaluate each recommendation text so that we can obtain accurate results.\n" \
                    "Note that you should output an average relevance score and an average quality score for each user, using the form 'Overall Average Quality Score: (). Overall Average Quality Score: ()'.You don't have to output score details for all recommandation. \n"

    # eval relevant, quality and matching
    task_subgroup['4-2'] = "Please note that your evaluation should be based on the above criteria and should be conducted as accurately as possible. Please carefully evaluate each recommendation text so that we can obtain accurate results.\n" \
                    "Note that you should output an average relevance score, an average quality score and an average matching score for each user, using the form 'Overall Average Relevant Score: (). Overall Average Quality Score: (). Overall Average Matching Score: ()'.You don't have to output score details for all recommandation. \n"

    # eval quality and matching
    task_subgroup['4-3'] = "Please note that your evaluation should be based on the above criteria and should be conducted as accurately as possible. Please carefully evaluate each recommendation text so that we can obtain accurate results.\n" \
                    "Note that you should output an average quality score and an average matching score for each user, using the form 'Overall Average Quality Score: (). Overall Average Matching Score: ()'.You don't have to output score details for all recommandation. \n"

    task_subgroup['4-4'] = 'Note that you should output the result {yes or no} in the first line of your reply. '
    return task_subgroup


def geticbuexample():
    task_subgroup = {}

    task_subgroup['5-1'] = 'Example: \n' \
                            'User Query: Dog food \n' \
                            'Rewritten Query 1: Pet food \n' \
                            'Rewritten Query 2: Food for dogs \n' \
                            'Rewritten Query 3: Canine nutrition \n' \
                            'Rewritten Query 4: Dog meal options \n' \
                            'User Query: Best smartphone \n' \
                            'Rewritten Query 1: Recommended top smartphones \n' \
                            'Rewritten Query 2: Which phone is the best? \n' \
                            'Rewritten Query 3: High-performance smartphone recommendations \n' \
                            'Rewritten Query 4: Most cost-effective mobile phone \n'

    task_subgroup['5-2'] = 'Example: \n' \
                            'Item title: stocked glass candy jar crystal sugar bowl with glass lid candy box \n' \
                            'Item category: 100003885_storage bottles & jars \n' \
                            'Prediction: 100003313_dishes & jars \n' \
                            'Match: Yes \n' \
                            'Item title: 2021 New Muted Stripe Handle Waterproof Quilted Wellies Shearling Lined Rainboots Kids Fashion Wellingtons \n' \
                            'Item category: 3220804_rain boots \n' \
                            'Prediction: 127734134_men\'s socks \n' \
                            'Match: No \n' \

    task_subgroup['5-3'] = 'Example: \n' \
                            'Input: \n' \
                            'query: stocked glass candy jar crystal sugar bowl with glass lid candy box \n' \
                            'category: 100003885_storage bottles & jars \n' \
                            'Output: Yes, 1 \n' \
                            'Input: \n' \
                            'query: stocked glass candy jar crystal sugar bowl with glass lid candy box \n' \
                            'category: 617_Hair Dryer \n' \
                            'Output: Yes, 0 \n' \
                            'Input: \n' \
                            'query: 0150475 \n' \
                            'category: 617_Hair Dryer \n' \
                            'Output: No \n'

    return task_subgroup


def geticbutemplate():

    task_subgroup = {}

    task_subgroup['q-1'] = "Please rate the quality of the following 10 generated recommendation texts. Please note that this evaluation focuses on potential issues related to multilingualism, sentence order, spelling errors, and similar issues.\n" \
                            "recommendation texts:{} \n" \
                            "Please rate each generated text according to the following criteria:\n" \
                            "Multilingualism: Meets/does not meet common language standards\n" \
                            "Sentence order: Correct/incorrect\n" \
                            "Spelling errors: Present/not present\n" \
                            "Please note that this evaluation focuses specifically on the above three issues. When evaluating each text, please adhere to the evaluation criteria and provide accurate scores. \n"

    # prompt2 chatGPT不太愿意回答，很奇怪
    task_subgroup['q-2'] = "You are a evaluater of Alibaba.com. Please evaluate the quality of 10 generated recommendation phrases for a recommendation model, and determine whether there are any errors in expression. \n" \
                            "Input recommendation phrases: {}. \n" \
                            "Output: A quality score for each recommendation phrase, ranging from 0 to 1, where 0 indicates completely inaccurate and 1 indicates completely accurate. \n" \
                            "Evaluation criteria: The quality of the recommendation phrases should be evaluated based on the following criteria: 1) grammatical and spelling errors, 2) semantic errors. \n" \
                            "Evaluation method: You should assign a score of 0 to 1 for each recommendation phrase. Evaluation results should be summarized using a standardized evaluation method to ensure comparability and consistency. \n"

    # prompt3 是prompt1的改进版，chatGPT可以给出对应的分数，勉勉强强
    task_subgroup['q-3'] = "Please rate the quality of the following 10 generated recommendation texts. Please note that this evaluation focuses on potential issues related to 1)multilingualism, 2)semantic errors, 3)spelling errors, 4）fluency and similar issues.\n" \
                            "recommendation texts:{} \n" \
                            "Please rate each generated text according to the following criteria:\n" \
                            "Multilingualism: Meets/does not meet common language standards\n" \
                            "semantic errors: Correct/incorrect\n" \
                            "Spelling errors: Present/not present\n" \
                            "Fluency: Fluent/not fluent\n" \
                            "Please note that this evaluation focuses specifically on the above three issues. When evaluating each text, please ranging from 0 to 1, where 0 indicates completely inaccurate and 1 indicates completely accurate. If needed, provide explaination. \n" \
                            "After evaluation, calculate average scores for each metrics."

    task_subgroup['r-1'] = "Evaluate the relevance of 10 recommendation texts generated by a recommendation model to a user's interaction history. You will be given the user's interaction history and 10 recommendation texts generated by the model. \n" \
                            "User's interaction history: {} \n" \
                            "recommendation texts: {}\n" \
                            "Output: A relevance score for each recommendation text with respect to the user's interaction history. \n" \
                            "Evaluation Criteria: We will evaluate the relevance of the recommendation texts with respect to the user's interaction history based on the following criteria: 1) lexical similarity, 2) topic matching, and 3) historical behavior matching.\n" \
                            "Evaluation Method: We request you to give a score between 0 and 1 for each recommendation text with respect to the user's interaction history. 0 represents low relevance, whereas 1 represents high relevance.\n" \
                            "Please note that your evaluation should be based on the above criteria and should be conducted as accurately as possible. Please carefully evaluate each recommendation text so that we can obtain accurate results.\n" \
                            "After evaluation, calculate average scores for each metrics."

    task_subgroup['r-2'] = "Evaluate the relevance of 10 recommendation texts generated by a recommendation model to a user's interaction history. You will be given the user's interaction history and 10 recommendation texts generated by the model. \n" \
                            "Input: User's interaction history followed by recommendation texts for each user. \n" \
                            "Output: A relevance score for each recommendation text with respect to the user's interaction history. \n" \
                            "Evaluation Criteria: We will evaluate the relevance of the recommendation texts with respect to the user's interaction history based on the following criteria: 1) lexical similarity, 2) topic matching, and 3) historical behavior matching.\n" \
                            "Evaluation Method: We request you to give a score between 0 and 1 for each recommendation text with respect to the user's interaction history. 0 represents low relevance, whereas 1 represents high relevance.\n" \
                            "Please note that your evaluation should be based on the above criteria and should be conducted as accurately as possible. Please carefully evaluate each recommendation text so that we can obtain accurate results.\n" \
                            "After evaluation, calculate average scores for each metrics."

    # quality and relevant
    task_subgroup['e-1'] = "You will be given the interaction history and recommendation texts generated by the model for {} users. \n" \
                                "Evaluate the quality and relevance of 10 recommendation texts generated by a recommendation model to users' interaction history. \n" \
                                "Input: User's interaction history followed by recommendation texts for each user. \n" \
                                "Output: An average relevance score and an average quality score for 10 recommendation texts for each user, use 'Overall Average Relevance Score:' to output the scores. " \
                                "Then calculate the average relevance score and average quality score for all users, use 'Overall Average Quality Score:' to output the scores\n" \
                                "Evaluation Criteria: We will evaluate the relevance of the recommendation texts with respect to the user's interaction history based on the following criteria: 1) lexical similarity, 2) topic matching, and 3) historical behavior matching. " \
                                "We will evaluate the relevance of the recommendation texts by focusing on potential issues related to 1)multilingualism, 2)semantic errors, 3)spelling errors, 4）fluency and similar issues.\n" \
                                "Evaluation Method: We request you to give a score between 0 and 1 for each recommendation text. 0 represents low score, whereas 1 represents high score.\n" \
                                "Please note that your evaluation should be based on the above criteria and should be conducted as accurately as possible. Please carefully evaluate each recommendation text so that we can obtain accurate results.\n" \
                                "Note that you should output an average relevance score and an average quality score for each user, using the form 'Overall Average Quality Score: (). Overall Average Quality Score: ()'.You don't have to output score details for all recommandation. \n"

    # quality, relevant and matching
    task_subgroup['e-2'] = all_eval_head['1-2'] + all_eval_criteria['2-2'] + all_eval_method['3-3'] + all_eval_note['4-2']

    # quality and matching
    task_subgroup['e-3'] = all_eval_head['1-3'] + all_eval_criteria['2-3'] + all_eval_method['3-4'] + all_eval_note['4-3']

    task_subgroup['e-4'] = all_eval_head['1-4'] + all_eval_example['5-2'] + all_eval_note['4-4']

    task_subgroup['e-5'] = all_eval_head['1-5'] + all_eval_example['5-3']

    return task_subgroup


all_eval_head = getEvalHead()
all_eval_criteria = getEvalCriteria()
all_eval_method = getEvalMethod()
all_eval_note = getEvalNote()
all_eval_example = geticbuexample()
eval_tasks = geticbutemplate()


